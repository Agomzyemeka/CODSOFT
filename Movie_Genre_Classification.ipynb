{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asL5Yy2EWrhZ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyRdDYkqAKN4"
      },
      "source": [
        "## Before you start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG37Y-xgV4-F"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\AGOMOH\\Desktop\\CODSOFT\\.venv\\Scripts\\python.exe\n",
            "Error importing TensorFlow: No module named 'tensorflow'\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5008\\3861258801.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error importing TensorFlow:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Check if CUDA is available\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)\n",
        "\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    print(\"TensorFlow version:\", tf.__version__)\n",
        "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "except ImportError as e:\n",
        "    print(\"Error importing TensorFlow:\", e)\n",
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "is_cuda_available = torch.cuda.is_available()\n",
        "print(\"Is CUDA available:\", is_cuda_available)\n",
        "\n",
        "# Get the number of CUDA devices\n",
        "cuda_device_count = torch.cuda.device_count()\n",
        "print(\"CUDA device count:\", cuda_device_count)\n",
        "\n",
        "if is_cuda_available:\n",
        "    for i in range(cuda_device_count):\n",
        "        print(f\"CUDA Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txX-2fFTXIZk"
      },
      "source": [
        "Cell 1: Import necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "w-P4vSvHXN06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\AGOMOH\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\AGOMOH\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "import time\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Ensure output images directory exists\n",
        "os.makedirs('images', exist_ok=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAtVEDiUXenX"
      },
      "source": [
        "Cell 2: Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "f04ETiguXeLW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset with specified encoding\n",
        "def load_data(file_path, is_train=True, encoding='utf-8'):\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding=encoding) as file:\n",
        "        for line in file.readlines():\n",
        "            parts = line.strip().split(' ::: ')\n",
        "            if is_train:\n",
        "                data.append({'ID': parts[0], 'TITLE': parts[1], 'GENRE': parts[2], 'DESCRIPTION': parts[3]})\n",
        "            else:\n",
        "                data.append({'ID': parts[0], 'TITLE': parts[1], 'DESCRIPTION': parts[2]})\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Specify the file paths\n",
        "train_data_path = os.path.join('dataset', 'Genre Classification Dataset', 'train_data.txt')\n",
        "test_data_path = os.path.join('dataset', 'Genre Classification Dataset', 'test_data.txt')\n",
        "test_solutions_path = os.path.join('dataset', 'Genre Classification Dataset', 'test_data_solution.txt')\n",
        "\n",
        "# Load the data with UTF-8 encoding\n",
        "train_data = load_data(train_data_path, is_train=True, encoding='utf-8')\n",
        "test_data = load_data(test_data_path, is_train=False, encoding='utf-8')\n",
        "test_solutions = pd.read_csv(test_solutions_path, sep=' ::: ', engine='python', header=None, names=['ID', 'GENRE'], encoding='utf-8')\n",
        "\n",
        "# If UTF-8 encoding doesn't work, try with 'latin-1'\n",
        "# train_data = load_data(train_data_path, is_train=True, encoding='latin-1')\n",
        "# test_data = load_data(test_data_path, is_train=False, encoding='latin-1')\n",
        "# test_solutions = pd.read_csv(test_solutions_path, sep=' ::: ', header=None, names=['ID', 'GENRE'], encoding='latin-1')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDkWSJ0FY51W"
      },
      "source": [
        "Cell 3: Preprocess the textual data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9WOWYIVkY7HV"
      },
      "outputs": [],
      "source": [
        "# Preprocess the textual data\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "train_data['DESCRIPTION'] = train_data['DESCRIPTION'].apply(preprocess_text)\n",
        "test_data['DESCRIPTION'] = test_data['DESCRIPTION'].apply(preprocess_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew0H90Yhe3M7"
      },
      "source": [
        "Cell 4: Vectorize text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Adw0eLBEe3_I"
      },
      "outputs": [],
      "source": [
        "# Vectorize text data\n",
        "def vectorize_text(train_data, test_data, text_list):\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Combine train, test, and additional text data for vectorization\n",
        "    combined_data = pd.concat([train_data['DESCRIPTION'], test_data['DESCRIPTION'], pd.Series(text_list)])\n",
        "\n",
        "    # Fit and transform TF-IDF vectorizer\n",
        "    X_tfidf = tfidf_vectorizer.fit_transform(combined_data)\n",
        "\n",
        "    # Split into train and test data\n",
        "    X_train_tfidf = X_tfidf[:len(train_data)]\n",
        "    X_test_tfidf = X_tfidf[len(train_data):(len(train_data) + len(test_data))]\n",
        "    X_text_list_tfidf = X_tfidf[(len(train_data) + len(test_data)):]\n",
        "\n",
        "    return X_train_tfidf, X_test_tfidf, X_text_list_tfidf, tfidf_vectorizer\n",
        "\n",
        "text_list = train_data['DESCRIPTION'].tolist()  # Text list used in the original preprocessing step\n",
        "X_train_tfidf, X_test_tfidf, X_text_list_tfidf, tfidf_vectorizer = vectorize_text(train_data, test_data, text_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au5MDfdDwUX4"
      },
      "source": [
        "Cell 5: Split data for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "04dIxQIiwaNM"
      },
      "outputs": [],
      "source": [
        "# Split data for training and testing\n",
        "y_train = train_data['GENRE']\n",
        "y_test = test_solutions['GENRE']\n",
        "# Define genres\n",
        "genres = train_data['GENRE'].unique()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SPJ_Z79wgN4"
      },
      "source": [
        "Cell 6: Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cY-jNmPFwhCR"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5008\\3184771222.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnb_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mnb_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5008\\3184771222.py\u001b[0m in \u001b[0;36mtrain_models\u001b[1;34m(X_train_tfidf, y_train)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlr_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mlr_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0msvm_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AGOMOH\\Desktop\\CODSOFT\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1612\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m             )\n\u001b[1;32m-> 1614\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarm_start_coef_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarm_start_coef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1615\u001b[0m         )\n\u001b[0;32m   1616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AGOMOH\\Desktop\\CODSOFT\\.venv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1863\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1865\u001b[0m         \u001b[1;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AGOMOH\\Desktop\\CODSOFT\\.venv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1792\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AGOMOH\\Desktop\\CODSOFT\\.venv\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AGOMOH\\Desktop\\CODSOFT\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    810\u001b[0m                 \u001b[0mjac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 812\u001b[1;33m                 \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"iprint\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"gtol\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"maxiter\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    813\u001b[0m             )\n\u001b[0;32m    814\u001b[0m             n_iter_i = _check_optimize_result(\n",
            "\u001b[1;32mc:\\Users\\AGOMOH\\Desktop\\CODSOFT\\.venv\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m--> 624\u001b[1;33m                                 callback=callback, **options)\n\u001b[0m\u001b[0;32m    625\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
            "\u001b[1;32mc:\\Users\\AGOMOH\\Desktop\\CODSOFT\\.venv\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AGOMOH\\Desktop\\CODSOFT\\.venv\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfun_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36marray_equal\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AGOMOH\\Desktop\\CODSOFT\\.venv\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36marray_equal\u001b[1;34m(a1, a2, equal_nan)\u001b[0m\n\u001b[0;32m   2444\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2445\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mequal_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0ma2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2447\u001b[0m     \u001b[1;31m# Handling NaN values if equal_nan is True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2448\u001b[0m     \u001b[0ma1nan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma2nan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Model training\n",
        "def train_models(X_train_tfidf, y_train):\n",
        "    nb_model = MultinomialNB()\n",
        "    nb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    lr_model = LogisticRegression(max_iter=1000)\n",
        "    lr_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    svm_model = SVC()\n",
        "    svm_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    return nb_model, lr_model, svm_model\n",
        "\n",
        "nb_model, lr_model, svm_model = train_models(X_train_tfidf, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQpeZSpkwy_t"
      },
      "source": [
        "Cell 7: Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-OJ1Hxcwz9O"
      },
      "outputs": [],
      "source": [
        "# Model evaluation\n",
        "def evaluate_models(model, X_test_tfidf, y_test, model_name):\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n",
        "    plt.xlabel('Predicted Genre')\n",
        "    plt.ylabel('Actual Genre')\n",
        "    plt.title(f'Confusion Matrix for {model_name}')\n",
        "    plt.show()\n",
        "\n",
        "    # Save the confusion matrix as an image\n",
        "    plt.savefig(f'images/confusion_matrix_{model_name}.png')\n",
        "\n",
        "print(\"Evaluating Naive Bayes Model:\")\n",
        "evaluate_models(nb_model, X_test_tfidf, y_test, 'Naive Bayes')\n",
        "print(\"Evaluating Logistic Regression Model:\")\n",
        "evaluate_models(lr_model, X_test_tfidf, y_test, 'Logistic Regression')\n",
        "print(\"Evaluating Support Vector Machine Model:\")\n",
        "evaluate_models(svm_model, X_test_tfidf, y_test, 'SVM')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvpubZUnxCdB"
      },
      "source": [
        "Cell 8: Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLOIhERJxDZM"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning\n",
        "def tune_hyperparameters(model, param_grid, X_train_tfidf, y_train):\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train_tfidf, y_train)\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "nb_param_grid = {'alpha': [0.1, 1.0, 10.0]}\n",
        "lr_param_grid = {'C': [0.1, 1.0, 10.0], 'max_iter': [100, 500, 1000]}\n",
        "svm_param_grid = {'C': [0.1, 1.0, 10.0], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']}\n",
        "\n",
        "print(\"Tuning Naive Bayes Model:\")\n",
        "nb_model = tune_hyperparameters(nb_model, nb_param_grid, X_train_tfidf, y_train)\n",
        "print(\"Tuning Logistic Regression Model:\")\n",
        "lr_model = tune_hyperparameters(lr_model, lr_param_grid, X_train_tfidf, y_train)\n",
        "print(\"Tuning Support Vector Machine Model:\")\n",
        "svm_model = tune_hyperparameters(svm_model, svm_param_grid, X_train_tfidf, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm_a7S0rxIgh"
      },
      "source": [
        "Cell 9: Cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmM4H_IUxK0H"
      },
      "outputs": [],
      "source": [
        "# Cross-validation\n",
        "def perform_cross_validation(model, X_train_tfidf, y_train):\n",
        "    cv_scores = cross_val_score(model, X_train_tfidf, y_train, cv=5, scoring='accuracy')\n",
        "    print(\"Cross-Validation Scores:\", cv_scores)\n",
        "    print(\"Mean CV Accuracy:\", np.mean(cv_scores))\n",
        "\n",
        "print(\"Cross-Validation for Naive Bayes Model:\")\n",
        "perform_cross_validation(nb_model, X_train_tfidf, y_train)\n",
        "print(\"Cross-Validation for Logistic Regression Model:\")\n",
        "perform_cross_validation(lr_model, X_train_tfidf, y_train)\n",
        "print(\"Cross-Validation for Support Vector Machine Model:\")\n",
        "perform_cross_validation(svm_model, X_train_tfidf, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r5_O3ZlxhCC"
      },
      "source": [
        "Cell 10: Train ensemble models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itWu64pdxikb"
      },
      "outputs": [],
      "source": [
        "# Train ensemble models\n",
        "def train_ensemble_models(X_train_tfidf, y_train):\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "    gb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    return rf_model, gb_model\n",
        "\n",
        "rf_model, gb_model = train_ensemble_models(X_train_tfidf, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngC_XJN2xmbf"
      },
      "source": [
        "Cell 11: Evaluate ensemble models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOMbteuPxnp_"
      },
      "outputs": [],
      "source": [
        "print(\"Evaluating Random Forest Model:\")\n",
        "evaluate_models(rf_model, X_test_tfidf, y_test, 'Random Forest')\n",
        "print(\"Evaluating Gradient Boosting Model:\")\n",
        "evaluate_models(gb_model, X_test_tfidf, y_test, 'Gradient Boosting')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtICLxUOxrpY"
      },
      "source": [
        "Cell 12: Visualize test predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Wz-TNmOxtRW"
      },
      "outputs": [],
      "source": [
        "# Visualize test predictions\n",
        "def visualize_test_predictions(model, X_test_tfidf, tfidf_vectorizer, genres):\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'Description': tfidf_vectorizer.inverse_transform(X_test_tfidf),\n",
        "        'Predicted Genre': y_pred,\n",
        "        'Actual Genre': y_test\n",
        "    })\n",
        "\n",
        "    # Select a sample for visualization\n",
        "    sample_predictions = predictions_df.sample(10)\n",
        "\n",
        "    for idx, row in sample_predictions.iterrows():\n",
        "        print(f\"Description: {' '.join(row['Description'])}\")\n",
        "        print(f\"Predicted Genre: {row['Predicted Genre']}\")\n",
        "        print(f\"Actual Genre: {row['Actual Genre']}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "    # Save the sample predictions as an image\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "    table = ax.table(cellText=sample_predictions.values, colLabels=sample_predictions.columns, cellLoc='center', loc='center')\n",
        "    plt.savefig(f'images/sample_predictions_{model.__class__.__name__}.png')\n",
        "\n",
        "print(\"Visualizing Test Predictions for Naive Bayes Model:\")\n",
        "visualize_test_predictions(nb_model, X_test_tfidf, tfidf_vectorizer, genres)\n",
        "print(\"Visualizing Test Predictions for Logistic Regression Model:\")\n",
        "visualize_test_predictions(lr_model, X_test_tfidf, tfidf_vectorizer, genres)\n",
        "print(\"Visualizing Test Predictions for Support Vector Machine Model:\")\n",
        "visualize_test_predictions(svm_model, X_test_tfidf, tfidf_vectorizer, genres)\n",
        "print(\"Visualizing Test Predictions for Random Forest Model:\")\n",
        "visualize_test_predictions(rf_model, X_test_tfidf, tfidf_vectorizer, genres)\n",
        "print(\"Visualizing Test Predictions for Gradient Boosting Model:\")\n",
        "visualize_test_predictions(gb_model, X_test_tfidf, tfidf_vectorizer, genres)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_LhthS4xyS5"
      },
      "source": [
        "Cell 13: Save models and vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7UoGlcHxy4e"
      },
      "outputs": [],
      "source": [
        "# Save models and vectorizer\n",
        "def save_model(model, filename):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(model, file)\n",
        "\n",
        "save_model(nb_model, 'nb_model.pkl')\n",
        "save_model(lr_model, 'lr_model.pkl')\n",
        "save_model(svm_model, 'svm_model.pkl')\n",
        "save_model(rf_model, 'rf_model.pkl')\n",
        "save_model(gb_model, 'gb_model.pkl')\n",
        "save_model(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
